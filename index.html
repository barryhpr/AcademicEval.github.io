<!DOCTYPE html>
<html>

<head>
    <title>AcademicEval</title>
    <!-- <link rel="icon" href="website/img/mint-leaf-logo.png" type="image/icon type"> -->

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->
    
    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-C7GJ4FYMY9');
    </script>
    
    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <!-- <img src="website/img/mint-leaf-logo.png" alt="logo" width="40" height="40" /> -->
                            AcademicEval Benchmark: ...
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                              Tao Feng<sup>1*</sup>,
                            </span>
                              <span class="author-block">
                                Pengrui Han<sup>1,2*</sup>,
                              </span>
                                <span class="author-block">
                                  Guanyu Lin<sup>1,3*</sup>,
                                </span>
                                <span class="author-block">
                                  <a href="https://www.mit.edu/~geliu/" target="_blank">Ge Liu</a><sup>1</sup>,</span>
                                <span class="author-block">
                                  <span class="author-block">
                                    <a href="https://cs.stanford.edu/~jiaxuan/" target="_blank">Jiaxuan You</a><sup>1</sup></span>
                                  <span class="author-block">
                              
                                </div>
                           
                                <div class="is-size-5 publication-authors">
                                  <span class="author-block">
                                    <sup>1</sup>University of Illinois Urbana-Champaign,
                                    <sup>2</sup>Carleton College,
                                    <sup>3</sup>Carnegie Mellon University
                                  <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
                                </div>
              

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2309.10691" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/xingyaoww/mint-bench" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">

                                    <a href="https://barryhpr.github.io/thought_retriever_web.github.io/"
                                        class="btn btn-outline-dark" role="button">&#128194;
                                        Project Page</a>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">

                <h2 class="subtitle">
                    Note: ....
                </h2>
                <!-- <div id="new-table"></div> -->
                <div id="benchmark-table"></div>
                <br>
                <!-- <h2 class="subtitle">
                    <b>MINT</b> can measure different LLMs' ability to provide natural language feedback by measuring the benefit of their feedback (&Delta; Success Rate) to a fixed LLM (gpt-3.5-turbo-0613).
                </h2>
                <div id="benchmark-feedback-efficancy-table"></div> -->
                <!-- <br>
                <h2 class="subtitle">
                    Please refer to our <a href="https://github.com/xingyaoww/mint-bench">GitHub repo</a> to add your model to the leaderboard.
                </h2> -->
            </div>
        </div>
    </section>




    <section class="section" id="Dataset Card">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Dataset Card</h2>
                        <p>
                            For AcademicEval-abstract, in the single document setting, each case includes the paper title, abstract, and main content, excluding the abstract and conclusion. For the multiple document setting, we combine five such entries into one. For AcademicEval-related, each paper includes a title, its abstract, and a label indicating whether it is the original paper, the original paper's related work, or just a random paper under the same broader field.
                        </p>
                        <!-- <ul>
                            <li>Abs-Single
                            </li>
                            <li>
                                Abs-Multi
                            </li>
                            <li>
                                Related work
                            </li>
                        </ul> -->
                        <div style="text-align:center;">
                            <img src="website/img/DatasetCard.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <br>
                        </div>
                    </div>
                </div>
                <!--/ Visual Effects. -->

            </div>
    </section>


    <section class="section" id="Usage Instruction">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Usage Instruction</h2>
                        <p>
                            Here we offer detailed instructions for utilizing the datasets in the AcademicEval benchmark. 
                        </p>

                           <ul>
                            <li> <b> Abs-Single. </b> For the task of single paper abstract summarization, as shown in Figure (a), we issue a query: "Given a title, please write an abstract." We retrieve information from the paper's main content using this query. The Language Model (LLM) composes the abstract based on the retrieved information. Finally, we compare the LLM-generated abstract with the original abstract.

                            </li>

                            <li>
                                <b>Abs-Multi.</b> For multiple paper abstracts summarization task, shown in Figure (b), we provide the query that given the titles of the 5 papers, please write an abstract. Then we retrieve information from the main content of the 5 papers based on this query. We then ask the LLM to directly summarize the 5 papers' abstracts as the ground truth. The generated abstract is then compared with the ground truth. 
                            </li>

                            <li>
                                <b>Related work.</b>  In the related work task, as shown in Figure (c), we provide the LLM with a query: "Given the title and abstract of a paper (labeled 'Original Paper'), please write its related work section." Following this prompt, the LLM retrieves information from a collection of papers, each comprising a title and abstract. This collection is composed of works cited in the original paper's related works and additional random papers. The LLM generates the related works section based on this retrieved information. This generated section is then evaluated against directly asking the LLM to summarize the related works cited in the original paper.
                            </li>
                        </ul>
                        
                        <div style="text-align:center;">
                            <img src="website/img/usage.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <br>
                        </div>
                    </div>
                </div>
                <!--/ Visual Effects. -->

            </div>
    </section>

    <!-- <section class="section" id="Usage Instruction">
        <div class="container is-max-desktop">

            <div class="columns is-full-width"> -->

                <!-- Visual Effects. -->
                <!-- <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Usage Instruction</h2>
                        <p>
                            Explanation here
                        </p>
                        <ul>
                            <li>...</li>
                         
                        </ul>

                        <h3>Tool-augmented Task-Solving capabilities of LLMs</h3>
                        <div class="text-justify" id="tool-augmented">
                            <ul>
                                <li>
                                    We find all open-source models fall behind most commercial closed-source models in
                                    both success
                                    rate
                                    at k=5 and improvement rate (slope).
                                    <br>
                                    <button class="btn btn-outline-secondary btn-sm"
                                        id="visualize-sr-vs-k-open-behind-close">Visualize
                                        This</button>
                                </li>
                                <li>
                                    Absolute performance and improvement-per-turn (e.g., slope) scale with model size.
                                    <br>
                                    <div class="btn-group" role="group">
                                        <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button"
                                            id="visualize-sr-vs-k-scale-with-model-size-llama2-base">Visualize: LLaMA-2
                                            Base</button>
                                        <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button"
                                            id="visualize-sr-vs-k-scale-with-model-size-llama2-rlhf">LLaMA-2
                                            RLHF</button>
                                        <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button"
                                            id="visualize-sr-vs-k-scale-with-model-size-codellama-base">CodeLLaMA
                                            Base</button>
                                        <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button"
                                            id="visualize-sr-vs-k-scale-with-model-size-codellama-sift">CodeLLaMA
                                            SIFT</button>
                                    </div>
                                </li>

                                <li>
                                    SIFT on multi-turn data can potentially be helpful. <a
                                        href="https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md">Vicuna-v1.5
                                        (7B)</a>, which is a SIFT variant of LLaMA2 trained on ShareGPT conversations
                                    (most are multi-turn), exhibit stronger performance compared to LLaMA-2 (Base and RLHF)<sup><a
                                            href="#footnote-1" id="ref-footnote-1">1</a></sup>.
                                    We observe similar trend for <a href="https://github.com/OpenLemur/Lemur">Lemur-70b-chat-v1</a>, which continue pre-train LLaMA-2 (70B) on code intensive data followed by SIFT on multi-turn data.
                                    <br>
                                        <div class="btn-group" role="group">
                                            <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button"
                                                id="visualize-sr-vs-k-vicuna-better-than-llama">Visualize: Vicuna-v1.5 (7B)</button>
                                            <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button"
                                                id="visualize-sr-vs-k-lemur-better-than-llama">Lemur-v1 (70B)</button>
                                        </div>
                                </li>

                                <li>
                                    We find RLHF hurt LLM-tool multi-turn interaction on LLaMA-2 series. However, it's
                                    unclear if RLHF is problematic overall, or if the issue only arise when RLHF is primarily applied to
                                    single-turn data.
                                    <br>
                                    <button class="btn btn-outline-secondary btn-sm inline-vis-button"
                                        id="visualize-sr-vs-k-rlhf">Visualize This</button>
                                </li>
                            </ul>

                            <ol>
                                <li style="font-size: 0.8rem;" id="footnote-1">We find some performance degradation in
                                    Vicuna-v1.5
                                    (especially for the 13B one), potential due to training artifacts. We refer to paper
                                    Section 3.5
                                    for
                                    more details.</li>
                            </ol>

                        </div>

                        <button class="btn btn-outline-secondary btn-sm" id="visualize-sr-vs-k-all">Visualize All
                            Models</button>

                        <div class="chart-container" id="chart-k" style="display:block;margin:0 auto;">
                            <canvas id="chart-sr-vs-k"></canvas>
                        </div>

                        <h3>LLMs' Ability to Leverage Natural Language Feedback</h3>
                        <ul>
                            <li>
                                We find no significant difference between open- and closed-source models in terms of
                                &Delta;feedback.
                                <br>
                                <button class="btn btn-outline-secondary btn-sm inline-vis-button"
                                    id="visualize-feedback-sr-no-diff-open-close">Visualize
                                    This</button>

                            </li> -->
<!-- 
                            <li>
                                Similar to previous findings, we find that SIFT and RLHF hurt models' ability to
                                leverage feedback on CodeLLama (except 7B) and LLaMA-2, as they all have lower &Delta;feedback and Success Rate (with feedback) compared to their base variants.
                                Another two exceptions are Vicuna and Lemur-v1; We speculate using multi-turn conversations (ShareGPT) for SIFT contributes to these two exceptions.
                                <br>
                                <button class="btn btn-outline-secondary btn-sm inline-vis-button"
                                    id="visualize-feedback-sr-sift-rlhf">Visualize
                                    This</button>
                            </li>

                            <li>
                                Models hardly benefit from self-feedback. We find GPT-4-0613 using self-generated
                                feedback has
                                limited benefit: only decision-making has improved slightly.
                                <br>
                                <button class="btn btn-outline-secondary btn-sm inline-vis-button"
                                    id="visualize-feedback-sr-gpt-4-self">Visualize
                                    This</button>
                            </li>

                        </ul>



                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Choose task type
                                    to
                                    visualize:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="avg_micro">Micro
                                    Average</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="reasoning">Reasoning</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="decision_making">Decision-Making</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="code_generation">Code</button>
                            </div> -->


                            <!-- <div class="btn-group btn-group-toggle text-center sort-by-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Sort
                                    by:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedbacksr">Success
                                    Rate with GPT-4 Feedback</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-nofeedbacksr">Without
                                    Feedback</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedbackdelta">&Delta;
                                    Feedback</button>
                            </div>
                        </div>

                        <div class="chart-container" id="chart-feedback" style="position:relative;margin:0 auto;">
                            <canvas id="chart-sr-w-feedback" style="max-height: 100%;"></canvas>
                        </div>

                        <h3>LLMs' Ability to Provide Natural Language Feedback</h3> -->

                        <!-- <p>
                            In this section, we fixed the evaluated LLM (gpt-3.5-turbo-0613) and use different
                            LLMs to
                            <b>provide</b> language feedback.
                            This allows us to measure different LLMs' effectiveness in providing feedback.
                            <br>
                            We find that task-solving ability could be orthogonal to feedback-providing ability: LLM's
                            higher task-solving performance does not necessarily translate to better feedback-providing
                            capability and vice versa.
                            For example, despite performing the worst in solving tasks, CodeLLaMA (34B, SIFT) can provide feedback that improves the stronger GPT-3.5.
                        </p> -->

                        <!-- <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center feedback-provider-sort-by-selector"
                                data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Sort
                                    by:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedback-gain">
                                    Success Rate with Feedback
                                </button> -->
<!-- 
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedback-provider-perf">
                                    Feedback Provider's Performance
                                </button>
                            </div>
                        </div> -->

                        <!-- <div class="chart-container" id="chart-feedback-p" style="display:block;margin:0 auto;">
                            <canvas id="chart-feedback-provider"></canvas>
                        </div>

                    </div>
                </div>
            </div>
    </section> -->



    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <!-- <pre><code>@misc{wang2023mint,
    title={MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback},
    author={Xingyao Wang and Zihan Wang and Jiateng Liu and Yangyi Chen and Lifan Yuan and Hao Peng and Heng Ji},
    year={2023},
    eprint={2309.10691},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}</code></pre> -->
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
